# -*- coding: utf-8 -*-
"""Fake_News.ipynb"

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1StI4EZedpXnrZnNyRYx82rg9vFtCdJh0
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np
import re
import spacy
from spacy.lang.en import English
nlp = spacy.load('en_core_web_sm')
import matplotlib.pyplot as plt
import matplotlib.gridspec as gridspec
from spacy.lang.en.stop_words import STOP_WORDS
from sklearn import feature_extraction, model_selection, naive_bayes, pipeline, manifold, preprocessing, feature_selection, metrics

"""# Functions"""

def json_splitter(column):
  ''' Function that identifies the incorrectly stored .json files among the rows
  of the data frame and creates the id list with their indexes;
  splits the strings into separate comments and their labels and saves them into
  a data frame. 
  '''
  inds = []
  labels = []
  texts = []
  pattern = r'\t.+?\n\d+.json\t\w+-?\w+?\t'
  labe_pattern = r'.json\t(.+?)\t'
  text_pattern = r'.+\t.+?\n\d+.json\t\w+-?\w+?\t'
  lab_dict = {'barely-true': 0, 'false': 1, 'pants-fire': 1, 'half-true': 2, 
              'mostly-true': 3, 'not-known': 4, 'true': 5}
  for ind in column.index:

    string = column[ind]
    
    if len(re.findall(pattern, string))>0:
      inds.append(ind)
      aux_labels = re.findall(labe_pattern, string)
      num_labels = [lab_dict[l] for l in aux_labels]
      labels = labels + num_labels

      aux_texts = re.findall(text_pattern, string)
      for text in aux_texts:
        text = re.sub(r'\t.+?\n\d+.json\t\w+-?\w+?\t','', text)
        texts.append(text)
  data = {'Labels': labels,
          'Text': texts
          }
  df = pd.DataFrame(data)
  df['text_len'] = df.Text.apply(len)
  return df, inds

def spacy_counter(df, type_, text_column, target_column):
    '''Functions that return a number of parts of speech or entetis from a 
    string.
    '''

    def stop_words_count(text):
      doc = nlp(text)
      n_stop = [t.text for t in doc  if t.text in STOP_WORDS]
    
      return len(n_stop)
    
    def partofSpeachRec(text,tag):
        doc = nlp(text)
        pos = [token.pos_ for token in doc]
        return pos.count(tag)
    def entetiesRecog(text, tag):
        doc = nlp(text)
        labels = [ent.label_ for ent in doc.ents]
        return labels.count(tag)
        
    ps_dict = {'ADJ': 'adjectives', 'ADP': 'prepositions', 'ADV': 'adverb',
              'NOUN': 'nouns', 'NUM': 'numerals', 'PRON': 'pronouns', 
               'PROPN': 'proper nouns', 'VERB': 'verbs'}
    ent_dict = {'PERSON': 'People, including fictional',
                'GPE': 'Countries, cities, states.', 
                'DATE': 'Absolute or relative dates or periods',
                'TIME': '	Times smaller than a day', 
                'ORDINAL': 'Ordinal numbers', 'CARDINAL': 'Cardinal numbers'}
    
    stop_dict = {'STOP_WORDS': 'Number of stop words'}
     
    if type_ == 'stop_words':
      
      function = stop_words_count
      dictionary = stop_dict

    elif type_ == 'part_of_speech':
        
        function = partofSpeachRec
        dictionary = ps_dict

    elif type_ == 'entety_label':
        
        function = entetiesRecog
        dictionary = ent_dict

    for i in dictionary.keys():
      if i == 'STOP_WORDS':
        df[i] = df[text_column].apply(lambda x: function(x))
      else:
        df[i] = df[text_column].apply(lambda x: function(x, i))
      plot_test=df[i].hist(by=df[target_column], sharex=True,bins = 15)
      plt.suptitle("The %s distribution by label"% (dictionary[i]))
        
    return df


def text_cleaner(string):
  '''Function that converts a string to lowercase and removes punctuations,
   numbers and characters and then strips it;
   removes Stopwords and long spaces;
   performs Lemmatisation.
  '''
  string = re.sub(r'-', ' ', str(string))
  string = re.sub(r'[^\w\s]', '', str(string).lower().strip())
  string = re.sub(r'\d+', '', string)
  doc = nlp(string)
  lems = [i.lemma_ for i in doc if ((i.text not in STOP_WORDS) & (len(i.lemma_) > 2))]
  text = " ".join(lems)
  text = re.sub(' +', ' ', text)
  return text.strip()

def statsByLabel(df, columns, target_column):
  for i in columns:
    tot_col = [i, target_column]
    print(df[tot_col].groupby(target_column).mean())

"""# Loading and cleaning data"""

train_data = pd.read_csv('/content/drive/My Drive/Participants_Data_WH20/Train.csv')
test_data = pd.read_csv('/content/drive/My Drive/Participants_Data_WH20/Test.csv')
sub = pd.read_csv('/content/drive/My Drive/Participants_Data_WH20/sample submission.csv')

train_data.shape

train_data.head()

"""**Identifying and removing duplicated rows**"""

train_data.duplicated().sum()

train_data = train_data.drop_duplicates()

"""After removing the duplicated rows, duplicated Text rows with different Labels were identified. These occurrence will be challenging as they might confuse the algorythm."""

train_data[train_data['Text'].duplicated(keep = False)]

"""**Analysing the length of the comments**

As the plot shows there are some obvious outliers among the lengths of the comments which should be further investigated.
"""

train_data['text_len'] = train_data.Text.apply(len)

import seaborn as sns
sns.displot(train_data, x="text_len", binwidth=1)

train_data.text_len.describe()

"""Apparently those are .json files which were not stored properly and represent multiple comments with their labels and some additional information, such as text tag, etc. in the same row."""

train_data.Text[train_data.text_len > 500].values

"""To address this problem we will use json_splitter function created to drop those rows and append the data frame with the 'clean' comments and their labels."""

df_aux, ids = json_splitter(train_data.Text)
train_data = train_data[~train_data.index.isin(ids)]
train_data = pd.concat([train_data, df_aux], axis = 0, ignore_index = True)

"""# Text Analysis

**Target variable**

*For more readability we are going the replace the 
integer values with their definitions.*
"""

label_dict = {0: 'Barely-True', 1: 'False', 2: 'Half-True', 
                  3: 'Mostly-True', 4: 'Not-Known', 5: 'True'}
train_data['label_text'] = train_data['Labels'].map(label_dict)

"""As the below plot shows there are 6 categories to be predicted. 

*   8% of the data, labeled as 'Not-Known', will present the main challenge at the stage of the prediction, as each of its comments belongs to one of the rest of the labels and therefore will confuse the model.
*   Even though we could potentially call it a balanced data set, some of the other labels aren't very clear/different from each other, such as Half-True and Barely-True. Their ambiguousness will add nothing but noise to the model.
"""

train_data.label_text.value_counts(normalize = True)

sns.countplot(train_data.label_text)
plt.xlabel('Labels')

"""**Part of speech analysis**

According to the plots and statistics below

*   fake news have fewer adjectives(ADJ), nouns, prepositions(ADP) and adverbs(ADV), while having more verbs, which means that there is more 'action' focus of the fake news;
*   interestingly on average there are fewer pronouns used by those who fake news, that minimizes references to themselves. A person who is lying tends not to use 'we' and 'I', i.e. tends not to use personal pronouns; 
*   Even though on average there are more notions proper nouns(PROPN) in fake news, numbers are more common among real news(NUM) which implies more details in the latter.
"""

ps = spacy_counter(train_data, 'part_of_speech', 'Text', 'label_text')

c = ['ADJ', 'ADP', 'ADV', 'NOUN', 'NUM', 'PRON', 'PROPN', 'VERB']
statsByLabel(train_data, c, 'label_text')

"""**Entety analysis**

Even though famous people a slightly more common to be discussed in fake news, when it comes to more specific details such as time, geographic location(GPE), date and numbers (both ordinal and cardinal), real news contain more of it.
"""

check_ent = spacy_counter(train_data, 'entety_label', 'Text', 'label_text')

c = ['PERSON', 'GPE', 'DATE', 'TIME', 'ORDINAL', 'CARDINAL']
statsByLabel(train_data, c, 'label_text')

"""**Stop words analysis**

As for the stop words, on average there are fewer of them in fake news which make it more concise and brief.
"""

spacy_counter(train_data, 'stop_words', 'Text', 'label_text')

c = ['STOP_WORDS']
statsByLabel(train_data, c, 'label_text')

"""# Preprocessing data

We cleaned the text by converting a string to lowercase and removing punctuations, numbers and characters and then striping it.
After Stopwords and long spaces were removed we performed Lemmatisation.
"""

train_data['clean_text'] = train_data.Text.apply(text_cleaner)

"""# Splitting the data into train and test"""

dtf_train, dtf_test = model_selection.train_test_split(train_data, 
                                                       test_size=0.15)

y_train = dtf_train['Labels'].values
y_test = dtf_test['Labels'].values

"""# Bag-of-Words

**Feature Engineering**

We are going use the Tf-Idf vectorizer with a limit of 50,000 words (so the length of my vocabulary will be 50k), capturing unigrams (i.e. 'San' and 'Francisco') and bigrams (i.e. 'San Francisco').
"""

vectorizer = feature_extraction.text.TfidfVectorizer(max_features = 50000, ngram_range=(1,2))

"""Let's use vectorizer on the preprocessed corpus of the train set to extract a vocabulary and create the feature matrix."""

corpus = dtf_train['clean_text']
vectorizer.fit(corpus)
X_train = vectorizer.transform(corpus)
dic_vocabulary = vectorizer.vocabulary_

X_train.shape

sns.heatmap(
    X_train.todense()[:,np.random.randint(0,X_train.shape[1],100)]==0, vmin=0, vmax=1, cbar=False
    ).set_title('Sparse Matrix Sample')

"""# Feature selection

Chi-square test measures dependence between stochastic variables, so using this function 'weeds out' the features that are the most likely to be independent of class and therefore irrelevant for classification.

I am going to treat each category as binary (for example, the 'Fake' news is 1 for the Fake news and 0 for the others)
"""

y = dtf_train['Labels']
X_names = vectorizer.get_feature_names()
p_value_limit = 0.85

dtf_features = pd.DataFrame()
for cat in np.unique(y):
    chi2, p = feature_selection.chi2(X_train, y==cat)
    dtf_features = dtf_features.append(pd.DataFrame(
                   {"feature":X_names, "score":1-p, "y":cat}))
    dtf_features = dtf_features.sort_values(["y","score"], 
                    ascending=[True,False])
    dtf_features = dtf_features[dtf_features["score"]>p_value_limit]
X_names = dtf_features["feature"].unique().tolist()

len(dtf_features)

"""We reduced the number of features from 7500 to 5910 by keeping the most statistically relevant ones. Let's print them out by label."""

for cat in np.unique(y):
   print("# {}:".format(cat))
   print("  . selected features:",
         len(dtf_features[dtf_features["y"]==cat]))
   print("  . top features:", ",".join(
dtf_features[dtf_features["y"]==cat]["feature"].values[:10]))
   print(" ")

"""No it's time to refit the vectorizer on the corpus by giving this new set of words as input. That will produce a smaller feature matrix and a shorter vocabulary."""

vectorizer = feature_extraction.text.TfidfVectorizer(vocabulary = X_names)
vectorizer.fit(corpus)
X_train = vectorizer.transform(corpus)
dic_vocabulary = vectorizer.vocabulary_

X_train.shape

sns.heatmap(
    X_train.todense()[:,np.random.randint(0,X_train.shape[1],100)]==0, vmin=0, vmax=1, cbar=False
    ).set_title('Sparse Matrix Sample')

"""# Machine learning model - Naive Bayes

Naive Bayes algorithm: a probabilistic classifier that makes use of Bayes’ Theorem, a rule that uses probability to make predictions based on prior knowledge of conditions that might be related. This algorithm is the most suitable for such large dataset as it considers each feature independently, calculates the probability of each category, and then predicts the category with the highest probability.
"""

classifier = naive_bayes.MultinomialNB()

"""For the training purposes we will use the feature matrix and then test it on the transformed test set. I need to build a scikit-learn pipeline: a sequential application of a list of transformations and a final estimator."""

## pipeline
model = pipeline.Pipeline([("vectorizer", vectorizer),  
                           ("classifier", classifier)])
## train classifier
model["classifier"].fit(X_train, y_train)
## test
X_test = dtf_test['clean_text'].values
predicted = model.predict(X_test)
predicted_prob = model.predict_proba(X_test)

"""# Model Evaluation"""

classes = np.unique(y_test)
y_test_array = pd.get_dummies(y_test, drop_first=False).values
    
## Accuracy, Precision, Recall
accuracy = metrics.accuracy_score(y_test, predicted)
auc = metrics.roc_auc_score(y_test, predicted_prob, 
                            multi_class="ovr")
print("Accuracy:",  round(accuracy,2))
print("Auc:", round(auc,2))
print("Detail:")
print(metrics.classification_report(y_test, predicted))
    
## Plot confusion matrix
cm = metrics.confusion_matrix(y_test, predicted)
fig, ax = plt.subplots()
sns.heatmap(cm, annot=True, fmt='d', ax=ax, cmap=plt.cm.Blues, 
            cbar=False)
ax.set(xlabel="Pred", ylabel="True", xticklabels=classes, 
       yticklabels=classes, title="Confusion matrix")
plt.yticks(rotation=0)