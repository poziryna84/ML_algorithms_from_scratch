{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom wordcloud import WordCloud\nfrom itertools import chain \nfrom collections import Counter\nfrom bs4 import BeautifulSoup\nimport string\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nimport spacy\nnlp = spacy.load('en_core_web_lg')\nimport re\nfrom nltk.tag.perceptron import PerceptronTagger\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import f1_score, accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix, plot_confusion_matrix\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport requests\nimport urllib.request\nimport time\nfrom bs4 import BeautifulSoup","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(filename)\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"../input/train.tsv\", sep=\"\\t\")\ntest = pd.read_csv(\"../input/test.tsv\", sep=\"\\t\")\nsample = pd.read_csv(\"../input/sampleSubmission.csv\", sep=\"\\t\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train.shape)\nprint(train.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(9,6))\nsns.countplot(y=train.Sentiment, order = train.Sentiment.value_counts().iloc[:5].index)\nplt.title('The distribution of the Sentiment')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def ent_replace(string):\n    '''Function that takes a string as an input and returns the string with entities replaced with the corresponding labels \n    from Spacy package.\n    '''\n    doc = nlp(string)\n    for ent in doc.ents:\n        if ent.label_ == 'PERSON':\n            string = string.replace(ent.text, 'famousperson')\n    return string.lower()\n\ndef reduce_lengthening(text):\n    '''Function that eliminates more than two repeat letters from a word.\n    '''\n    pattern = re.compile(r\"(.)\\1{2,}\")\n    return pattern.sub(r\"\\1\\1\", text)\nresult = string.punctuation \ndef text_prepro(text):\n    '''Function that performs text preprocessing like spelling correction, 'cleaning' a string from stop words, digits, \n    accents, one letter word; entities replacement, stemming, etc.\n    '''\n    stop_words = set(stopwords.words('english'))\n    text = reduce_lengthening(text)\n    review_text = BeautifulSoup(text).get_text()\n    text = ent_replace(review_text)\n    text = text.lower()\n    text = text.replace(\"movie\", \"film\").replace(\"ca n't\", \"cannot\").replace(\"wo n't\", \"will not\")\n    text = text.replace(\"n't\", \"not\").replace(\"!\", \"auxexcl\")\n    text = ''.join([l if l not in result else ' ' for l in text]).replace('  ', ' ')\n    word_tokens = word_tokenize(text) \n    lemmatizer = WordNetLemmatizer()\n    lem_n = [lemmatizer.lemmatize(word) for word in word_tokens]\n    lem_a = [lemmatizer.lemmatize(word, pos='a') for word in lem_n]\n    lem_v = [lemmatizer.lemmatize(word, pos='v') for word in lem_a]\n    lemmed_words = [word for word in lem_v if word.isalpha() and (word not in stop_words or (word == 'not'or word == 'no')) and len(word)>1]\n    lem_string = ' '.join(lemmed_words)\n    return lem_string","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['phrase_mod'] = train['Phrase'].apply(text_prepro)\ntest['phrase_modif'] = test['Phrase'].apply(text_prepro)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"cloud_comment_words = WordCloud(background_color=\"black\").generate(\n    ' '.join(list(train['phrase_mod'][train['Sentiment'] == 0])))\nplt.imshow(cloud_comment_words, interpolation='bilinear') \nplt.axis(\"off\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cloud_comment_words = WordCloud(background_color=\"black\").generate(\n    ' '.join(list(train['phrase_mod'][train['Sentiment'] == 1])))\nplt.imshow(cloud_comment_words, interpolation='bilinear') \nplt.axis(\"off\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cloud_comment_words = WordCloud(background_color=\"black\").generate(\n    ' '.join(list(train['phrase_mod'][train['Sentiment'] == 2])))\nplt.imshow(cloud_comment_words, interpolation='bilinear') \nplt.axis(\"off\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cloud_comment_words = WordCloud(background_color=\"black\").generate(\n    ' '.join(list(train['phrase_mod'][train['Sentiment'] == 3])))\nplt.imshow(cloud_comment_words, interpolation='bilinear') \nplt.axis(\"off\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cloud_comment_words = WordCloud(background_color=\"black\").generate(\n    ' '.join(list(train['phrase_mod'][train['Sentiment'] == 4])))\nplt.imshow(cloud_comment_words, interpolation='bilinear') \nplt.axis(\"off\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_freq = Counter(chain.from_iterable(list(train['phrase_mod'].str.split())))\nmost_common = word_freq.most_common()\n\nword_freq_t = Counter(chain.from_iterable(list(train['phrase_mod'].str.split())))\ntotal = word_freq_t.most_common()\n\nword_freq_0 = Counter(chain.from_iterable(list(train['phrase_mod'][train['Sentiment'] == 0].str.split())))\nnegative = word_freq_0.most_common()\n\nword_freq_1 = Counter(chain.from_iterable(list(train['phrase_mod'][train['Sentiment'] == 1].str.split())))\nmod_negative = word_freq_1.most_common()\n\nword_freq_2 = Counter(chain.from_iterable(list(train['phrase_mod'][train['Sentiment'] == 2].str.split())))\nneutral = word_freq_2.most_common()\n\nword_freq_3 = Counter(chain.from_iterable(list(train['phrase_mod'][train['Sentiment'] == 3].str.split())))\nmod_positive = word_freq_3.most_common()\n\nword_freq_4 = Counter(chain.from_iterable(list(train['phrase_mod'][train['Sentiment'] == 4].str.split())))\npositive = word_freq_4.most_common()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"negative[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mod_negative[:15]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"neutral[:15]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mod_positive[:15]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"positive[:15]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nvectorizer_cv = CountVectorizer(ngram_range=(1, 3))\nX_cv = vectorizer_cv.fit_transform(train['phrase_mod'])\ny_cv = train['Sentiment']\nX_train_cv, X_test_cv, y_train_cv, y_test_cv = train_test_split(X_cv, y_cv, test_size=0.1, \n                                                    stratify = y_cv, random_state=42)\nprint(X_train_cv.shape)\nclf_cv = MultinomialNB()\nclf_cv.fit(X_train_cv, y_train_cv)\n\nprediction_cv = clf_cv.predict(X_test_cv)\nprint(accuracy_score(y_test_cv, prediction_cv))\ncf_matrix_cv = confusion_matrix(y_test_cv, prediction_cv)\nsns.heatmap(cf_matrix_cv/np.sum(cf_matrix_cv), annot=True, \n            fmt='.2%', cmap='Blues')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class_labels = clf_cv.classes_\n\nfeature_names = vectorizer_cv.get_feature_names()\n\n# Zip the feature names together with the coefficient array and sort by weights: feat_with_weights\nfeat_with_weights = sorted(zip(clf_cv.coef_[0], feature_names))\n\n# Print the first class label and the top 20 feat_with_weights entries\nprint('Top 20 lowest values (less predictive features)')\nprint()\nprint(class_labels[0], feat_with_weights[:20])\nprint()\nprint('*******************************************************************************************************************************')\nprint()\n# Print the second class label and the bottom 20 feat_with_weights entries\nprint('top 20 highest values (highest predictive features)')\nprint()\nprint(class_labels[1], feat_with_weights[-20:])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"matr = vectorizer_cv.transform(test['phrase_modif'])\ntest['Sentiment'] = clf_cv.predict(matr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['Sentiment'].value_counts(normalize = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['Sentiment'].value_counts(normalize = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = test[['PhraseId', 'Sentiment']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.display import FileLink","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv('mult_nom.csv', index=False)\nFileLink(r'mult_nom.csv')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}