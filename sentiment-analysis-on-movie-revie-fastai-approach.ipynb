{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom sklearn.model_selection import train_test_split\nimport fastai\nfrom fastai import *\nfrom fastai.text import * \n\nfrom functools import partial\n\nimport os","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"! pip install torch_nightly -f https://download.pytorch.org/whl/nightly/cu92/torch_nightly.html\n! pip install fastai","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(filename)\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"../input/train.tsv\", sep=\"\\t\")\ntest = pd.read_csv(\"../input/test.tsv\", sep=\"\\t\")\nsample = pd.read_csv(\"../input/sampleSubmission.csv\", sep=\"\\t\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.DataFrame({'label':df.Sentiment, 'text':df.Phrase})# df for deep learnimg approach","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df.shape)\nprint(df.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(9,6))\nsns.countplot(y=df.label, order = df.label.value_counts().iloc[:5].index)\nplt.title('The distribution of the Sentiment')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Fastai NLP approach","execution_count":null},{"metadata":{"trusted":true},"cell_type":"markdown","source":"*As text can't directly be transformed into numbers to be fed into a model. The first thing we need to do is to preprocess our data so that we change the raw texts to lists of words, or tokens (a step that is called tokenization) then transform these tokens into numbers (a step that is called numericalization). These numbers are then passed to embedding layers that will convert them in arrays of floats before passing them through a model.*\n\n*You can find on the web plenty of Word Embeddings to directly convert your tokens into floats. Those word embeddings have generally be trained on a large corpus such as wikipedia. Following the work of ULMFiT, the fastai library is more focused on using pre-trained Language Models and fine-tuning them. *\n\n*The library is structured around three steps:*\n\n* Get your data preprocessed and ready to use in a minimum amount of code,\n* Create a language model with pretrained weights that you can fine-tune to your dataset,\n* Create other models such as classifiers on top of the encoder of the language model.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Let’s clean our text by retaining only alphabets and removing everything else.\nI will get rid of the stopwords from our text data. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['text'] = df['text'].str.replace(\"[^a-zA-Z]\", \" \")\nstop_words = set(stopwords.words('english'))\n# tokenization \ntokenized_doc = df['text'].apply(lambda x: x.split())\n\n# remove stop-words \ntokenized_doc = tokenized_doc.apply(lambda x: [item for item in x if item not in stop_words])\n\n# de-tokenization \ndetokenized_doc = [] \nfor i in range(len(df)): \n    t = ' '.join(tokenized_doc[i]) \n    detokenized_doc.append(t) \n\ndf['text'] = detokenized_doc","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*First let’s split our dataset we prepared earlier into training and validation sets in a 80:20 ratio.*","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_trn, df_val = train_test_split(df[['label', 'text']], stratify = df['label'], test_size = 0.3, random_state = 12)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Language Model","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"*To get a DataBunch quickly, there are also several factory methods depending on how our data is structured. They are all detailed in text.data, here we'll use the method from_df of the TextLMDataBunch (to get the data ready for a language model). *","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data_lm = TextLMDataBunch.from_df(train_df = df_trn, valid_df = df_val, path = \"\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Fine-tuning a language model","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"*We can use the data_lm object we created earlier to fine-tune a pretrained language model. fast.ai has an English model with an AWD-LSTM architecture available that we can download. We can create a learner object that will directly create a model, download the pretrained weights and be ready for fine-tuning.*","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"learn = language_model_learner(data_lm,  arch = AWD_LSTM, pretrained = True, drop_mult = 0.5)# WT103_FWD, reducing drop_mult will avoid underfitting\n# https://stackoverflow.com/questions/57240057/not-able-to-use-fastais-pretrained-model-urls-wt103","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*First let´s find and vizualize the range of the learning rates with the corresponding losses.*","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.lr_find()\nlearn.recorder.plot(skip_end = 15)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*To fine tune the last layers we are going to go for 1e-02 learning rate because that´s where the loss line is going down. *","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.fit_one_cycle(1, 1e-02, moms = (0.8, 0.7))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.save('fit_head')\nlearn.load('fit_head')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*To complete fine tuning we are going to unfreeze and launch a new training with ten epochs this time:*","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.unfreeze()\nlearn.fit_one_cycle(4, 1e-03, moms = (0.8, 0.7)) # 1e should be a bit higher than in the first tuning, moms the same moms = (0.8, 0.7), epchs now 4","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.save('fine_tuned')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*To evaluate your language model, you can run the **Learner.predict** method and specify the number of words you want it to guess.*","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.predict(\"I like this movie because\", n_words=10, temperature = 1.1, min_p = 0.001)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*It is very important after all to save this encoder to use it for classification later.*","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.save_encoder('fine_tuned_encoder')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Classifier.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"*To get the DataBunch for our classifier we'll use the from_df method of TextClasDataBunch (to get the data ready for a text classifier) classes.*\n\n*This does all the necessary preprocessing behind the scene. For the classifier, we also pass the vocabulary (mapping from ids to words) that we want to use: this is to ensure that data_clas will use the same dictionary as data_lm.*\n\n*In general, batch size of 32 is a good starting point, and you should also try with 64, 128, and 256. Other values (lower or higher) may be fine for some data sets, but the given range is generally the best to start experimenting with.*","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data_clas = TextClasDataBunch.from_df(path = \"\", train_df = df_trn, valid_df = df_val, vocab = data_lm.train_ds.vocab, bs = 64)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Let's inspect the data_clas we've just created:*","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data_clas.show_batch()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"https://docs.fast.ai/text.transform.html#BaseTokenizer\n**\nThe rules are all listed below, here is the meaning of the special tokens:\n\nUNK (xxunk) is for an unknown word (one that isn't present in the current vocabulary)\n\nPAD (xxpad) is the token used for padding, if we need to regroup several texts of different lengths in a batch\n\nBOS (xxbos) represents the beginning of a text in your dataset\n\nFLD (xxfld) is used if you set mark_fields=True in your TokenizeProcessor to separate the different fields of texts (if your texts are loaded from several columns in a dataframe)\n\nTK_MAJ (xxmaj) is used to indicate the next word begins with a capital in the original text\n\nTK_UP (xxup) is used to indicate the next word is written in all caps in the original text\n\nTK_REP (xxrep) is used to indicate the next character is repeated n times in the original text (usage xxrep n {char})\n\nTK_WREP(xxwrep) is used to indicate the next word is repeated n times in the original text (usage xxwrep n {word})**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data_clas.vocab.itos[:20]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"*We now use the data_clas object we created earlier to build a classifier with our fine-tuned encoder. The learner object can be done in a single line.*","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"learn = text_classifier_learner(data_clas, arch = AWD_LSTM, drop_mult=0.5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.load_encoder('fine_tuned_encoder')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nlearn.freeze()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.lr_find()\nlearn.recorder.plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.fit_one_cycle(1, 1e-02, moms= (0.8, 0.7))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.save('first')\nlearn.load('first')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we are going to unfreeze and train only the last two layers (freeze_to(-2)):","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.freeze_to(-2)\nlearn.fit_one_cycle(1, slice(1e-2/(2.6**4), 1e-3), moms = (0.8, 0.7))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.save('second')\nlearn.load('second')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Unfreeze and train the next layer: (freeze_to(-3))","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.freeze_to(-3)\nlearn.fit_one_cycle(1, slice(5e-3/(2.6**4), 5e-3), moms = (0.8, 0.7))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.save('third')\nlearn.load('third')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Unfreeze the whole thing and train it:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.unfreeze()\nlearn.fit_one_cycle(2, slice(1e-3/(2.6**4), 1e-3), moms = (0.8, 0.7))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.save('last')\nlearn.load('last')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.predict('intermittently pleasing mostly routine')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Making Predictions on the validation set.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test = pd.DataFrame({'text':test.Phrase})# df for deep learnimg approach","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test['text'] = df_test['text'].str.replace(\"[^a-zA-Z]\", \" \")\n\n# tokenization \ntokenized_doc_test = df_test['text'].apply(lambda x: x.split())\n\n# remove stop-words \ntokenized_doc_test = tokenized_doc_test.apply(lambda x: [item for item in x if item not in stop_words])\n\n# de-tokenization \ndetokenized_doc_test = [] \nfor i in range(len(df_test)): \n    t = ' '.join(tokenized_doc_test[i]) \n    detokenized_doc_test.append(t) \n\ndf_test['text'] = detokenized_doc_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dd = df_test.text.value_counts().to_frame().reset_index()\nfor i in dd['index']:\n    print(i)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dd.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.predict('n')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = dd['index'].apply(lambda x: learn.predict(x))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}